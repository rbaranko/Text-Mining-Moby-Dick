{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Text from Moby Dick\n",
    "\n",
    "Data Source: \"Moby Dick\" by Herman Melville | 'moby.txt' file in project files\n",
    "\n",
    "This project utilizes text mining techniques to draw insights and analysis from the text found in \"Moby Dick\". The purpose of the project is to demonstrate basic anaysis of text which can be applied to other text and text formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Moby Dick by Herman Melville 1851]\\n\\n\\nETYMOLOGY.\\n\\n(Supplied by a Late Consumptive Usher to a Grammar School)\\n\\nThe pale Usher--threadbare in coat, heart, body, and brain; I see him\\nnow.  He was ever dusting his old lexicons and grammars, with a queer\\nhandkerchief, mockingly embellished with all the gay flags of all the\\nknown nations of the world.  He loved to dust his old grammars; it\\nsomehow mildly reminded him of his mortality.\\n\\n\"While you take in hand to school others, and to teach them by what\\nname a whale-fish is to be called in our tongue leaving out, through\\nignorance, the letter H, which almost alone maketh the signification\\nof the word, you deliver that which is not true.\" --HACKLUYT\\n\\n\"WHALE. ... Sw. and Dan. HVAL.  This animal is named from roundness\\nor rolling; for in Dan. HVALT is arched or vaulted.\" --WEBSTER\\'S\\nDICTIONARY\\n\\n\"WHALE. ... It is more immediately from the Dut. and Ger. WALLEN;\\nA.S. WALW-IAN, to roll, to wallow.\" --RICHARDSON\\'S DICTIONARY\\n\\nKETOS,               GRE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby_raw[0:1000] # raw text imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby_tokens = nltk.word_tokenize(moby_raw) # tokenize words to clean and prepare text for analysis\n",
    "text1 = nltk.Text(moby_tokens)\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tokens (words and punctuation symbols) are in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255018"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique tokens (unique words and punctuation) does the text have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20754 unique tokens\n",
      "16899 unique tokens after lemmatizing verbs\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "print('{} unique tokens'.format(len(set(text1))))\n",
    "def lem():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "print('{} unique tokens after lemmatizing verbs'.format(lem())) # grouping verbs with same root (i.e. \"running\" & \"runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08138249064771899"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(nltk.word_tokenize(moby_raw))) / len(nltk.word_tokenize(moby_raw)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that about 92% of tokens used are repeated at least once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of tokens is 'whale'or 'Whale'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43%\n"
     ]
    }
   ],
   "source": [
    "def whale_count():\n",
    "    text_1 = [x.lower() for x in text1]\n",
    "    whale_c = text_1.count('whale')\n",
    "    return whale_c / len(nltk.word_tokenize(moby_raw))  * 100\n",
    "print(str(round(whale_count(),2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 20 most frequently occurring (unique) tokens in the text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2111),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_20():\n",
    "    s = list(set(text1))\n",
    "    f = []\n",
    "    for c in s:\n",
    "        count = c , text1.count(c)\n",
    "        f.append(count)\n",
    "    final = sorted(f, key=lambda x:x[1], reverse=True)\n",
    "    return final[:20]\n",
    "top_20()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What tokens have a length of greater than 5 and frequency of more than 150?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_5_150():\n",
    "    t = FreqDist(text1)\n",
    "    f = []\n",
    "    for x in t:\n",
    "        if t[x] > 150 and len(x) > 5:\n",
    "            f.append(x)\n",
    "    return sorted(f)\n",
    "filter_5_150()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest word and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest word is twelve-o'clock-at-night and has 23 characters\n"
     ]
    }
   ],
   "source": [
    "def longest():\n",
    "    t = list(FreqDist(text1))\n",
    "    sw = sorted(t, key=len, reverse=True)\n",
    "    return sw[0] , len(sw[0])\n",
    "word, length = longest()\n",
    "print('The longest word is {} and has {} characters'.format(word, length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What unique words have a frequency of more than 2000? What is their frequency (not including numbers or punctuation)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2111, 'I')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_words():\n",
    "    t = FreqDist(text1)\n",
    "    f = []\n",
    "    for x in t:\n",
    "        if t[x] > 2000 and x.isalpha():\n",
    "            f.append((t[x], x))\n",
    "    f.sort(key=lambda x:x[0], reverse=True)\n",
    "    return f\n",
    "top_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average number of tokens per sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_tokens():\n",
    "    sentences = nltk.sent_tokenize(moby_raw)\n",
    "    l = []\n",
    "    for s in sentences:\n",
    "        c = len(nltk.word_tokenize(s))\n",
    "        l.append(c)\n",
    "    return np.array(l).mean()\n",
    "round(avg_tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "Legend for part of speech tagging found [here](https://www.guru99.com/pos-tagging-chunking-nltk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 32730), ('IN', 28658), ('DT', 25870), (',', 19204), ('JJ', 17619)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos():\n",
    "    pos = nltk.pos_tag(text1)\n",
    "    ls = [x[1] for x in pos]\n",
    "    l = FreqDist(ls)\n",
    "    f = []\n",
    "    for t in l:\n",
    "        f.append((t, l[t]))\n",
    "    final = sorted(f, key=lambda x:x[1], reverse=True)\n",
    "    return final[:5]\n",
    "pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
